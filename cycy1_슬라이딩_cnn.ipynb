{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPWhvppWiEqPbGWRXlNquOb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NZgJ0Ep_cX23"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import librosa\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv1D, MaxPooling1D\n","from keras.utils import to_categorical\n","import glob\n","import os\n","from sklearn.model_selection import train_test_split\n","\n","# 주파수 특성 추출 함수 (이전과 동일)\n","max_length = 15\n","\n","def extract_features(audio_file, max_length):\n","    y, sr = librosa.load(audio_file, sr=None)\n","    if len(y) < max_length:\n","        y = np.pad(y, (0, max_length - len(y)), 'constant')\n","    elif len(y) > max_length:\n","        y = y[:max_length]\n","    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n","    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n","    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n","    features = np.concatenate((mfccs.flatten(), chroma.flatten(), spectral_contrast.flatten()))\n","    return features\n","\n","def get_files_from_dir(path, file_format):\n","    # 주어진 경로와 그 하위 경로에서 특정 형식의 파일들을 찾습니다.\n","    return glob.glob(os.path.join(path, f'**/*.{file_format}'), recursive=True)\n","\n","#승연부탁\n","from keras.layers import BatchNormalization\n","\n","# 모델 정의\n","model=Sequential()\n","\n","model.add(Conv1D(filters=128,kernel_size=5,strides=1,padding=\"same\",activation=\"relu\",input_shape=(X_train.shape[1], 1)))\n","model.add(BatchNormalization())\n","model.add(MaxPooling1D(pool_size=5,strides=2,padding='same'))\n","\n","model.add(Conv1D(filters=64,kernel_size=5,strides=1,padding=\"same\",activation=\"relu\"))\n","model.add(BatchNormalization())\n","model.add(MaxPooling1D(pool_size=5,strides=2,padding='same'))\n","\n","model.add(Conv1D(filters=32,kernel_size=5,strides=1,padding=\"same\",activation=\"relu\"))\n","model.add(BatchNormalization())\n","model.add(MaxPooling1D(pool_size=5,strides=2,padding='same'))\n","\n","model.add(Dropout(0.3))\n","model.add(Flatten())\n","\n","# 더 큰 Dense 계층\n","model.add(Dense(units=len(set(np.concatenate((ai_labels, human_labels))))*10, activation=\"relu\"))\n","# Dropout 추가\n","model.add(Dropout(0.3))\n","\n","# 출력 계층 - 클래스 수와 동일한 유닛 수가 필요합니다.\n","# softmax 활성화 함수를 사용하여 각 클래스에 대한 확률을 출력합니다.\n","output_units = len(set(np.concatenate((ai_labels, human_labels))))\n","if output_units == 2:\n","    # 이진 분류의 경우 한 개의 출력 유닛과 시그모이드 활성화 함수를 사용할 수 있습니다.\n","    model.add(Dense(units = 1, activation = 'sigmoid'))\n","else:\n","    # 다중 클래스 분류의 경우 각 클래스에 대한 하나의 출력 유닛과 소프트맥스 활성화 함수를 사용합니다.\n","    model.add(Dense(units = output_units, activation = 'softmax'))\n","\n","# 컴파일 및 학습\n","if output_units == 2:\n","    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n","else:\n","    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","history=model.fit(X_train,y_train,batch_size=X_train.shape[0]//10 ,epochs = 30 ,validation_data=(X_test,y_test))\n","\n","\n","#===================================================================================\n","\n","\n","#조용한 부분 삭제 + 슬라이딩 + loss 해결\n","import librosa\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv1D, MaxPooling1D\n","from keras.utils import to_categorical\n","import glob\n","import os\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from keras.optimizers import SGD\n","\n","# 주파수 특성 추출 함수 (슬라이딩 윈도우 사용)\n","max_length = 5  # 각 슬라이딩 윈도우의 길이 (초)\n","window_hop = 0.5   # 윈도우를 이동시키는 간격 (초)\n","\n","def extract_features(audio_files, max_length, window_hop):\n","    features = []\n","    print(audio_files)\n","    for audio_file in audio_files:\n","        try:\n","            y, sr = librosa.load(audio_file, sr=None)\n","            sr_per_window = int(sr * max_length)\n","            sr_hop = int(sr * window_hop)\n","            # 슬라이딩 윈도우로 자르기\n","            for i in range(0, len(y) - sr_per_window + 1, sr_hop):\n","                window = y[i:i+sr_per_window]\n","                mfccs = librosa.feature.mfcc(y=window, sr=sr, n_mfcc=13)\n","                chroma = librosa.feature.chroma_stft(y=window, sr=sr)\n","                spectral_contrast = librosa.feature.spectral_contrast(y=window, sr=sr)\n","                window_features = np.concatenate((mfccs.flatten(), chroma.flatten(), spectral_contrast.flatten()))\n","                features.append(window_features)\n","        except Exception as e:\n","            print(f\"Error processing {audio_file}: {str(e)}\")\n","\n","    # 특성을 배열로 변환\n","    features_array = np.array(features)\n","    return features_array\n","\n","\n","\n","def get_files_from_dir(path, file_format):\n","    # 주어진 경로와 그 하위 경로에서 특정 형식의 파일들을 찾습니다.\n","    return glob.glob(os.path.join(path, f'**/*.{file_format}'), recursive=True)\n","\n","\n","ai_subfolders = ['sm', 'sy', 'mh']\n","ai_audio_files = []\n","for folder in ai_subfolders:\n","    ai_audio_files.extend(get_files_from_dir(f'/content/drive/MyDrive/PBL/ai_voice/ai_padding/{folder}', 'mp3'))\n","\n","human_subfolders = ['cy', 'sy', 'hs']\n","human_audio_files=[]\n","for folder in human_subfolders:\n","  human_audio_files.extend(get_files_from_dir(f'/content/drive/MyDrive/PBL/ai_voice/human_padding/{folder}', 'mp3'))\n","\n","\n","# AI 음성 특징 추출 및 레이블 생성\n","ai_features_train_test_fullset = extract_features(ai_audio_files, max_length=max_length, window_hop=window_hop)\n","ai_labels_train_test_fullset = np.zeros(len(ai_features_train_test_fullset))\n","\n","# 사람 음성 특징 추출 및 레이블 생성\n","human_features_train_test_fullset = extract_features(human_audio_files, max_length=max_length, window_hop=window_hop)\n","# 두 배열 중 하나의 열 크기를 다른 배열과 일치시키기\n","if ai_features_train_test_fullset.shape[1] != human_features_train_test_fullset.shape[1]:\n","    target_shape = (human_features_train_test_fullset.shape[0], ai_features_train_test_fullset.shape[1])\n","    human_features_train_test_fullset = np.resize(human_features_train_test_fullset, target_shape)\n","\n","human_labels_train_test_fullset = np.ones(len(human_features_train_test_fullset))\n","\n","# 학습 데이터와 테스트 데이터 결합 및 레이블 결합\n","X_train_test = np.vstack((ai_features_train_test_fullset, human_features_train_test_fullset))\n","y_train_test = np.concatenate((ai_labels_train_test_fullset, human_labels_train_test_fullset))\n","\n","print(X_train_test.shape)\n","print(y_train_test.shape)\n","# 데이터를 학습 데이터와 검증 데이터로 분할\n","X_train,X_test,y_train,y_test = train_test_split(X_train_test,y_train_test,test_size=0.2)\n","\n","\n","# 모델 정의 및 컴파일\n","num_classes = len(set(np.concatenate((ai_labels_train_test_fullset, human_labels_train_test_fullset))))\n","\n","model = Sequential()\n","model.add(Conv1D(filters=64, kernel_size=5, strides=1,\n","                 padding=\"same\", activation=\"relu\", input_shape=(X_train.shape[1], 1)))\n","model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n","model.add(Dropout(0.25))  # Dropout 추가\n","model.add(Conv1D(filters=64, kernel_size=5, strides=1,\n","                 padding=\"same\", activation=\"relu\"))\n","model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n","model.add(Dropout(0.25))  # Dropout 추가\n","model.add(Flatten())\n","model.add(Dense(units=num_classes, activation=\"softmax\"))\n","\n","# 모델 컴파일 시 학습률 조정 및 EarlyStopping 추가\n","from keras.optimizers import Adam\n","from keras.callbacks import EarlyStopping\n","\n","opt = Adam(learning_rate=0.0001)\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10)  # 조기 종료 설정\n","\n","# 더 낮은 학습률과 모멘텀을 사용하는 SGD 최적화\n","optimizer = SGD(lr=0.001, momentum=0.9)\n","\n","model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 데이터 정규화\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# 모델 컴파일 시 학습률 조정\n","from keras.optimizers import Adam\n","\n","opt = Adam(learning_rate=0.0001)\n","model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# 라벨 변환 제거 (sparse_categorical_crossentropy 사용시 필요 없음)\n","# y_train = to_categorical(y_train)\n","# y_test = to_categorical(y_test)\n","\n","history=model.fit(X_train_scaled,y_train,batch_size=16,\n","                  epochs=30,\n","                  validation_data=(X_test_scaled,y_test),\n","                  callbacks=[early_stopping])  # EarlyStopping 콜백 추가\n","\n","\n","#===================================================================================\n","\n","\n","#조용한 부분 삭제 + 슬라이딩 + loss 해결\n","import librosa\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv1D, MaxPooling1D\n","from keras.utils import to_categorical\n","import glob\n","import os\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from keras.optimizers import SGD\n","\n","# 주파수 특성 추출 함수 (슬라이딩 윈도우 사용)\n","max_length = 5  # 각 슬라이딩 윈도우의 길이 (초)\n","window_hop = 0.5   # 윈도우를 이동시키는 간격 (초)\n","\n","def extract_features(audio_files, max_length, window_hop):\n","    features = []\n","    print(audio_files)\n","    for audio_file in audio_files:\n","        try:\n","            y, sr = librosa.load(audio_file, sr=None)\n","            sr_per_window = int(sr * max_length)\n","            sr_hop = int(sr * window_hop)\n","            # 슬라이딩 윈도우로 자르기\n","            for i in range(0, len(y) - sr_per_window + 1, sr_hop):\n","                window = y[i:i+sr_per_window]\n","                mfccs = librosa.feature.mfcc(y=window, sr=sr, n_mfcc=13)\n","                chroma = librosa.feature.chroma_stft(y=window, sr=sr)\n","                spectral_contrast = librosa.feature.spectral_contrast(y=window, sr=sr)\n","                window_features = np.concatenate((mfccs.flatten(), chroma.flatten(), spectral_contrast.flatten()))\n","                features.append(window_features)\n","        except Exception as e:\n","            print(f\"Error processing {audio_file}: {str(e)}\")\n","\n","    # 특성을 배열로 변환\n","    features_array = np.array(features)\n","    return features_array\n","\n","\n","\n","def get_files_from_dir(path, file_format):\n","    # 주어진 경로와 그 하위 경로에서 특정 형식의 파일들을 찾습니다.\n","    return glob.glob(os.path.join(path, f'**/*.{file_format}'), recursive=True)\n","\n","\n","ai_subfolders = ['sm', 'sy', 'mh']\n","ai_audio_files = []\n","for folder in ai_subfolders:\n","    ai_audio_files.extend(get_files_from_dir(f'/content/drive/MyDrive/PBL/ai_voice/ai_padding/{folder}', 'mp3'))\n","\n","human_subfolders = ['cy', 'sy', 'hs']\n","human_audio_files=[]\n","for folder in human_subfolders:\n","  human_audio_files.extend(get_files_from_dir(f'/content/drive/MyDrive/PBL/ai_voice/human_padding/{folder}', 'mp3'))\n","\n","\n","# AI 음성 특징 추출 및 레이블 생성\n","ai_features_train_test_fullset = extract_features(ai_audio_files, max_length=max_length, window_hop=window_hop)\n","ai_labels_train_test_fullset = np.zeros(len(ai_features_train_test_fullset))\n","\n","# 사람 음성 특징 추출 및 레이블 생성\n","human_features_train_test_fullset = extract_features(human_audio_files, max_length=max_length, window_hop=window_hop)\n","# 두 배열 중 하나의 열 크기를 다른 배열과 일치시키기\n","if ai_features_train_test_fullset.shape[1] != human_features_train_test_fullset.shape[1]:\n","    target_shape = (human_features_train_test_fullset.shape[0], ai_features_train_test_fullset.shape[1])\n","    human_features_train_test_fullset = np.resize(human_features_train_test_fullset, target_shape)\n","\n","human_labels_train_test_fullset = np.ones(len(human_features_train_test_fullset))\n","\n","# 학습 데이터와 테스트 데이터 결합 및 레이블 결합\n","X_train_test = np.vstack((ai_features_train_test_fullset, human_features_train_test_fullset))\n","y_train_test = np.concatenate((ai_labels_train_test_fullset, human_labels_train_test_fullset))\n","\n","print(X_train_test.shape)\n","print(y_train_test.shape)\n","# 데이터를 학습 데이터와 검증 데이터로 분할\n","X_train,X_test,y_train,y_test = train_test_split(X_train_test,y_train_test,test_size=0.2)\n","\n","\n","# 모델 정의 및 컴파일\n","num_classes = len(set(np.concatenate((ai_labels_train_test_fullset, human_labels_train_test_fullset))))\n","\n","model = Sequential()\n","model.add(Conv1D(filters=64, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\", input_shape=(X_train.shape[1], 1)))\n","model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n","model.add(Conv1D(filters=64, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\"))\n","model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n","model.add(Dropout(0.5))  # 더 많은 Dropout\n","model.add(Conv1D(filters=32, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\"))\n","model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n","model.add(Dropout(0.5))  # 더 많은 Dropout\n","model.add(Flatten())\n","model.add(Dense(units=num_classes, activation=\"softmax\"))\n","\n","# 더 낮은 학습률과 모멘텀을 사용하는 SGD 최적화\n","optimizer = SGD(lr=0.001, momentum=0.9)\n","\n","model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 데이터 정규화\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# 모델 컴파일 시 학습률 조정\n","from keras.optimizers import Adam\n","\n","opt = Adam(learning_rate=0.0001)\n","model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# 라벨 변환 제거 (sparse_categorical_crossentropy 사용시 필요 없음)\n","# y_train = to_categorical(y_train)\n","# y_test = to_categorical(y_test)\n","\n","history=model.fit(X_train_scaled,y_train,batch_size=16,epochs=30,validation_data=(X_test_scaled,y_test))\n","\n","\n","#=================================================================================\n","\n","\n","# AI 음성파일들\n","ai_subfolders = ['sm', 'sy', 'mh']\n","ai_audio_files = []\n","for folder in ai_subfolders:\n","    ai_audio_files.extend(get_files_from_dir(f'/content/drive/MyDrive/PBL/ai_voice/ai/{folder}', 'mp3'))\n","\n","# 사람 음성파일들\n","human_subfolders = ['cy', 'sy', 'hs']\n","human_audio_files = []\n","for folder in human_subfolders:\n","    human_audio_files.extend(get_files_from_dir(f'/content/drive/MyDrive/PBL/ai_voice/human/{folder}', 'mp3'))\n","\n","ai_features = [extract_features(audio_file,max_length) for audio_file in ai_audio_files]\n","human_features = [extract_features(audio_file,max_length) for audio_file in human_audio_files]\n","\n","ai_labels = np.zeros(len(ai_features))\n","human_labels = np.ones(len(human_features))\n","\n","X_train_test= np.vstack(ai_features + human_features)\n","y_train_test= np.concatenate((ai_labels, human_labels))\n","\n","print(\"Shape of X_train_test:\", X_train_test.shape)\n","print(\"Shape of y_train_test:\", y_train_test.shape)\n","\n","# 학습 데이터와 테스트 데이터 분리\n","X_train,X_test,y_train,y_test = train_test_split(X_train_test,y_train_test,test_size=0.2)\n","\n","\n","#==============================================================================\n","\n","\n","#조용한 부분삭제 버전+슬라이딩\n","import librosa\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv1D, MaxPooling1D\n","from keras.utils import to_categorical\n","import glob\n","import os\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","# 주파수 특성 추출 함수 (슬라이딩 윈도우 사용)\n","max_length = 5  # 각 슬라이딩 윈도우의 길이 (초)\n","window_hop = 0.5   # 윈도우를 이동시키는 간격 (초)\n","\n","def extract_features(audio_files, max_length, window_hop):\n","    features = []\n","    print(audio_files)\n","    for audio_file in audio_files:\n","        try:\n","            y, sr = librosa.load(audio_file, sr=None)\n","            sr_per_window = int(sr * max_length)\n","            sr_hop = int(sr * window_hop)\n","            # 슬라이딩 윈도우로 자르기\n","            for i in range(0, len(y) - sr_per_window + 1, sr_hop):\n","                window = y[i:i+sr_per_window]\n","                mfccs = librosa.feature.mfcc(y=window, sr=sr, n_mfcc=13)\n","                chroma = librosa.feature.chroma_stft(y=window, sr=sr)\n","                spectral_contrast = librosa.feature.spectral_contrast(y=window, sr=sr)\n","                window_features = np.concatenate((mfccs.flatten(), chroma.flatten(), spectral_contrast.flatten()))\n","                features.append(window_features)\n","        except Exception as e:\n","            print(f\"Error processing {audio_file}: {str(e)}\")\n","\n","    # 특성을 배열로 변환\n","    features_array = np.array(features)\n","    return features_array\n","\n","\n","\n","def get_files_from_dir(path, file_format):\n","    # 주어진 경로와 그 하위 경로에서 특정 형식의 파일들을 찾습니다.\n","    return glob.glob(os.path.join(path, f'**/*.{file_format}'), recursive=True)\n","\n","\n","ai_subfolders = ['sm', 'sy', 'mh']\n","ai_audio_files = []\n","for folder in ai_subfolders:\n","    ai_audio_files.extend(get_files_from_dir(f'/content/drive/MyDrive/PBL/ai_voice/ai_padding/{folder}', 'mp3'))\n","\n","human_subfolders = ['cy', 'sy', 'hs']\n","human_audio_files=[]\n","for folder in human_subfolders:\n","  human_audio_files.extend(get_files_from_dir(f'/content/drive/MyDrive/PBL/ai_voice/human_padding/{folder}', 'mp3'))\n","\n","\n","# AI 음성 특징 추출 및 레이블 생성\n","ai_features_train_test_fullset = extract_features(ai_audio_files, max_length=max_length, window_hop=window_hop)\n","ai_labels_train_test_fullset = np.zeros(len(ai_features_train_test_fullset))\n","\n","# 사람 음성 특징 추출 및 레이블 생성\n","human_features_train_test_fullset = extract_features(human_audio_files, max_length=max_length, window_hop=window_hop)\n","# 두 배열 중 하나의 열 크기를 다른 배열과 일치시키기\n","if ai_features_train_test_fullset.shape[1] != human_features_train_test_fullset.shape[1]:\n","    target_shape = (human_features_train_test_fullset.shape[0], ai_features_train_test_fullset.shape[1])\n","    human_features_train_test_fullset = np.resize(human_features_train_test_fullset, target_shape)\n","\n","human_labels_train_test_fullset = np.ones(len(human_features_train_test_fullset))\n","\n","# 학습 데이터와 테스트 데이터 결합 및 레이블 결합\n","X_train_test = np.vstack((ai_features_train_test_fullset, human_features_train_test_fullset))\n","y_train_test = np.concatenate((ai_labels_train_test_fullset, human_labels_train_test_fullset))\n","\n","print(X_train_test.shape)\n","print(y_train_test.shape)\n","# 데이터를 학습 데이터와 검증 데이터로 분할\n","X_train,X_test,y_train,y_test = train_test_split(X_train_test,y_train_test,test_size=0.2)\n","\n","\n","# 모델 정의 및 컴파일\n","num_classes = len(set(np.concatenate((ai_labels_train_test_fullset, human_labels_train_test_fullset))))\n","\n","model = Sequential()\n","model.add(Conv1D(filters=64, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\", input_shape=(X_train.shape[1], 1)))\n","model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n","model.add(Conv1D(filters=32, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\"))\n","model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n","model.add(Dropout(0.2))\n","model.add(Flatten())\n","model.add(Dense(units=num_classes, activation=\"softmax\"))  # num_classes를 클래스 수로 설정\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","\n","\n","X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n","y_train = to_categorical(y_train)\n","\n","X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n","y_test = to_categorical(y_test)\n","\n","history=model.fit(X_train,y_train,batch_size=16,epochs=30,validation_data=(X_test,y_test))\n","\n","\n","#==============================================================================\n","\n","\n","#슬라이딩거\n","import librosa\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv1D, MaxPooling1D\n","from keras.utils import to_categorical\n","import glob\n","import os\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","# 주파수 특성 추출 함수 (슬라이딩 윈도우 사용)\n","max_length = 5  # 각 슬라이딩 윈도우의 길이 (초)\n","window_hop = 0.5   # 윈도우를 이동시키는 간격 (초)\n","\n","def extract_features(audio_files, max_length, window_hop):\n","    features = []\n","    print(audio_files)\n","    for audio_file in audio_files:\n","        try:\n","            y, sr = librosa.load(audio_file, sr=None)\n","            sr_per_window = int(sr * max_length)\n","            sr_hop = int(sr * window_hop)\n","            # 슬라이딩 윈도우로 자르기\n","            for i in range(0, len(y) - sr_per_window + 1, sr_hop):\n","                window = y[i:i+sr_per_window]\n","                mfccs = librosa.feature.mfcc(y=window, sr=sr, n_mfcc=13)\n","                chroma = librosa.feature.chroma_stft(y=window, sr=sr)\n","                spectral_contrast = librosa.feature.spectral_contrast(y=window, sr=sr)\n","                window_features = np.concatenate((mfccs.flatten(), chroma.flatten(), spectral_contrast.flatten()))\n","                features.append(window_features)\n","        except Exception as e:\n","            print(f\"Error processing {audio_file}: {str(e)}\")\n","\n","    # 특성을 배열로 변환\n","    features_array = np.array(features)\n","    return features_array\n","\n","\n","\n","def get_files_from_dir(path, file_format):\n","    # 주어진 경로와 그 하위 경로에서 특정 형식의 파일들을 찾습니다.\n","    return glob.glob(os.path.join(path, f'**/*.{file_format}'), recursive=True)\n","\n","\n","ai_subfolders = ['sm', 'sy', 'mh']\n","ai_audio_files = []\n","for folder in ai_subfolders:\n","    ai_audio_files.extend(get_files_from_dir(f'/content/drive/MyDrive/PBL/ai_voice/ai/{folder}', 'mp3'))\n","\n","human_subfolders = ['cy', 'sy', 'hs']\n","human_audio_files=[]\n","for folder in human_subfolders:\n","  human_audio_files.extend(get_files_from_dir(f'/content/drive/MyDrive/PBL/ai_voice/human/{folder}', 'mp3'))\n","\n","\n","# AI 음성 특징 추출 및 레이블 생성\n","ai_features_train_test_fullset = extract_features(ai_audio_files, max_length=max_length, window_hop=window_hop)\n","ai_labels_train_test_fullset = np.zeros(len(ai_features_train_test_fullset))\n","\n","# 사람 음성 특징 추출 및 레이블 생성\n","human_features_train_test_fullset = extract_features(human_audio_files, max_length=max_length, window_hop=window_hop)\n","# 두 배열 중 하나의 열 크기를 다른 배열과 일치시키기\n","if ai_features_train_test_fullset.shape[1] != human_features_train_test_fullset.shape[1]:\n","    target_shape = (human_features_train_test_fullset.shape[0], ai_features_train_test_fullset.shape[1])\n","    human_features_train_test_fullset = np.resize(human_features_train_test_fullset, target_shape)\n","\n","human_labels_train_test_fullset = np.ones(len(human_features_train_test_fullset))\n","\n","# 학습 데이터와 테스트 데이터 결합 및 레이블 결합\n","X_train_test = np.vstack((ai_features_train_test_fullset, human_features_train_test_fullset))\n","y_train_test = np.concatenate((ai_labels_train_test_fullset, human_labels_train_test_fullset))\n","\n","print(X_train_test.shape)\n","print(y_train_test.shape)\n","# 데이터를 학습 데이터와 검증 데이터로 분할\n","X_train,X_test,y_train,y_test = train_test_split(X_train_test,y_train_test,test_size=0.2)\n","\n","\n","# 모델 정의 및 컴파일\n","num_classes = len(set(np.concatenate((ai_labels_train_test_fullset, human_labels_train_test_fullset))))\n","\n","model = Sequential()\n","model.add(Conv1D(filters=64, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\", input_shape=(X_train.shape[1], 1)))\n","model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n","model.add(Conv1D(filters=32, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\"))\n","model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n","model.add(Dropout(0.2))\n","model.add(Flatten())\n","model.add(Dense(units=num_classes, activation=\"softmax\"))  # num_classes를 클래스 수로 설정\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","\n","\n","X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n","y_train = to_categorical(y_train)\n","\n","X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n","y_test = to_categorical(y_test)\n","\n","history=model.fit(X_train,y_train,batch_size=16,epochs=30,validation_data=(X_test,y_test))\n","\n","\n","#=========================================================================\n","\n","\n","import librosa\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv1D, MaxPooling1D\n","from keras.utils import to_categorical\n","import glob\n","import os\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","# 주파수 특성 추출 함수 (슬라이딩 윈도우 사용)\n","max_length = 5  # 각 슬라이딩 윈도우의 길이 (초)\n","window_hop = 0.5   # 윈도우를 이동시키는 간격 (초)\n","\n","def extract_features(audio_files, max_length, window_hop):\n","    features = []\n","    print(audio_files)\n","    for audio_file in audio_files:\n","        try:\n","            y, sr = librosa.load(audio_file, sr=None)\n","            sr_per_window = int(sr * max_length)\n","            sr_hop = int(sr * window_hop)\n","            # 슬라이딩 윈도우로 자르기\n","            for i in range(0, len(y) - sr_per_window + 1, sr_hop):\n","                window = y[i:i+sr_per_window]\n","                mfccs = librosa.feature.mfcc(y=window, sr=sr, n_mfcc=13)\n","                chroma = librosa.feature.chroma_stft(y=window, sr=sr)\n","                spectral_contrast = librosa.feature.spectral_contrast(y=window, sr=sr)\n","                window_features = np.concatenate((mfccs.flatten(), chroma.flatten(), spectral_contrast.flatten()))\n","                features.append(window_features)\n","        except Exception as e:\n","            print(f\"Error processing {audio_file}: {str(e)}\")\n","\n","    # 특성을 배열로 변환\n","    features_array = np.array(features)\n","    return features_array\n","\n","def get_files_from_dir(path, file_format):\n","    # 주어진 경로와 그 하위 경로에서 특정 형식의 파일들을 찾습니다.\n","    return glob.glob(os.path.join(path, f'**/*.{file_format}'), recursive=True)\n","\n","ai_subfolders = ['sm', 'sy', 'mh']\n","ai_audio_files = []\n","for folder in ai_subfolders:\n","    ai_audio_files.extend(get_files_from_dir(f'/content/drive/MyDrive/PBL/ai_voice/ai_padding/{folder}', 'mp3'))\n","\n","human_subfolders = ['cy', 'sy', 'hs']\n","human_audio_files=[]\n","for folder in human_subfolders:\n","  human_audio_files.extend(get_files_from_dir(f'/content/drive/MyDrive/PBL/ai_voice/human_paddiing/{folder}', 'mp3'))\n","\n","# AI 음성 특징 추출 및 레이블 생성\n","ai_features_train_test_fullset = extract_features(ai_audio_files, max_length=max_length, window_hop=window_hop)\n","ai_labels_train_test_fullset = np.zeros(len(ai_features_train_test_fullset))\n","\n","# 사람 음성 특징 추출 및 레이블 생성\n","human_features_train_test_fullset = extract_features(human_audio_files, max_length=max_length, window_hop=window_hop)\n","# 두 배열 중 하나의 열 크기를 다른 배열과 일치시키기\n","if ai_features_train_test_fullset.shape[1] != human_features_train_test_fullset.shape[1]:\n","    target_shape = (human_features_train_test_fullset.shape[0], ai_features_train_test_fullset.shape[1])\n","    human_features_train_test_fullset = np.resize(human_features_train_test_fullset, target_shape)\n","\n","human_labels_train_test_fullset = np.ones(len(human_features_train_test_fullset))\n","\n","# 학습 데이터와 테스트 데이터 결합 및 레이블 결합\n","X_train_test = np.vstack((ai_features_train_test_fullset, human_features_train_test_fullset))\n","y_train_test = np.concatenate((ai_labels_train_test_fullset, human_labels_train_test_fullset))\n","\n","print(X_train_test.shape)\n","print(y_train_test.shape)\n","# 데이터를 학습 데이터와 검증 데이터로 분할\n","X_train, X_test, y_train, y_test = train_test_split(X_train_test, y_train_test, test_size=0.2)\n","\n","# 모델 정의 및 컴파일\n","num_classes = len(set(np.concatenate((ai_labels_train_test_fullset, human_labels_train_test_fullset))))\n","\n","model = Sequential()\n","model.add(Conv1D(filters=32, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\", input_shape=(X_train.shape[1], 1)))\n","model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n","model.add(Flatten())\n","model.add(Dense(units=num_classes, activation=\"softmax\"))  # num_classes를 클래스 수로 설정\n","\n","# 모델 컴파일\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 데이터 형태 변경\n","X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n","y_train = to_categorical(y_train)\n","\n","X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n","y_test = to_categorical(y_test)\n","\n","# 모델 학습\n","history = model.fit(X_train, y_train, batch_size=16, epochs=30, validation_data=(X_test, y_test))\n","\n","\n","#==========================================================================\n","\n","\n","import librosa\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv1D, MaxPooling1D\n","from keras.utils import to_categorical\n","import glob\n","import os\n","from sklearn.model_selection import train_test_split\n","\n","# 주파수 특성 추출 함수 (슬라이딩 윈도우 사용)\n","max_length = 5  # 각 슬라이딩 윈도우의 길이 (초)\n","window_hop = 0.5   # 윈도우를 이동시키는 간격 (초)\n","\n","def extract_features(audio_data):\n","    features = []\n","    for audio in audio_data:\n","        try:\n","            # 오디오 파일의 유효성을 검사\n","            if not librosa.util.valid_audio(audio):\n","                print(f\"Invalid audio file: {audio}\")\n","                continue\n","\n","            sr = librosa.get_samplerate(audio)\n","            mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n","            chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n","            spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n","            audio_features = np.concatenate((mfccs.flatten(), chroma.flatten(), spectral_contrast.flatten()))\n","            features.append(audio_features)\n","        except Exception as e:\n","            print(f\"Error processing audio: {str(e)}\")\n","\n","    # 특성을 배열로 변환\n","    features_array = np.array(features)\n","    return features_array\n","\n","\n","def get_files_from_dir(path, file_format):\n","    # 주어진 경로와 그 하위 경로에서 특정 형식의 파일들을 찾습니다.\n","    return glob.glob(os.path.join(path, f'**/*.{file_format}'), recursive=True)\n","\n","ai_subfolders = ['sm', 'sy', 'mh']\n","ai_audio_files = []\n","for folder in ai_subfolders:\n","    ai_audio_files.extend(get_files_from_dir(f'/content/drive/MyDrive/PBL/ai_voice/ai/{folder}', 'mp3'))\n","\n","human_subfolders = ['cy', 'sy', 'hs']\n","human_audio_files = []\n","for folder in human_subfolders:\n","  human_audio_files.extend(get_files_from_dir(f'/content/drive/MyDrive/PBL/ai_voice/human/{folder}', 'mp3'))\n","\n","import librosa\n","import numpy as np\n","import glob\n","import os\n","\n","cut_human_audio_data = []\n","cut_ai_audio_data = []\n","min_length = float('inf')  # 무한대로 초기화\n","\n","# 모든 오디오 파일을 최단 길이로 자르고 자른 데이터를 cut_audio_data 배열에 추가\n","for audio_file in human_audio_files:\n","    y, sr = librosa.load(audio_file, sr=None)\n","    if len(y) < min_length:\n","        min_length = len(y)\n","\n","for audio_file in human_audio_files:\n","    y, sr = librosa.load(audio_file, sr=None)\n","    if len(y) > min_length:\n","        start = (len(y) - min_length) // 2  # 중앙에서 시작하여 자릅니다.\n","        end = start + min_length\n","        cut_audio = y[start:end]\n","        cut_human_audio_data.append(cut_audio)\n","\n","min_length = float('inf')  # 무한대로 초기화\n","for audio_file in ai_audio_files:\n","    y, sr = librosa.load(audio_file, sr=None)\n","    if len(y) < min_length:\n","        min_length = len(y)\n","\n","for audio_file in ai_audio_files:\n","    y, sr = librosa.load(audio_file, sr=None)\n","    if len(y) > min_length:\n","        start = (len(y) - min_length) // 2  # 중앙에서 시작하여 자릅니다.\n","        end = start + min_length\n","        cut_audio = y[start:end]\n","        cut_ai_audio_data.append(cut_audio)\n","# 인간 음성 데이터를 WAV 파일로 저장\n","for i, audio in enumerate(cut_human_audio_data):\n","    output_path = f'human_audio_{i}.wav'  # 저장할 WAV 파일 경로\n","    librosa.output.write_wav(output_path, audio, sr=sr)\n","\n","# AI 음성 데이터를 WAV 파일로 저장\n","for i, audio in enumerate(cut_ai_audio_data):\n","    output_path = f'ai_audio_{i}.wav'  # 저장할 WAV 파일 경로\n","    librosa.output.write_wav(output_path, audio, sr=sr)\n","\n","print(cut_ai_audio_data)\n","# AI 음성 특징 추출 및 레이블 생성\n","ai_features_train_test_fullset = extract_features(cut_ai_audio_data)\n","ai_labels_train_test_fullset = np.ones(ai_features_train_test_fullset.shape[0])\n","\n","# 인간 음성 특징 추출 및 레이블 생성\n","human_features_train_test_fullset = extract_features(cut_human_audio_data)\n","human_labels_train_test_fullset = np.zeros(human_features_train_test_fullset.shape[0])\n","\n","# 데이터셋 합치기\n","X = np.concatenate((ai_features_train_test_fullset, human_features_train_test_fullset))\n","y = np.concatenate((ai_labels_train_test_fullset, human_labels_train_test_fullset))\n","print(human_labels_train_test_fullset.shape)\n","print(ai_features_train_test_fullset.shape)\n","# 데이터를 훈련 세트와 테스트 세트로 나누기\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True, stratify=y)\n","\n","# 모델 구축\n","model = Sequential()\n","model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Flatten())\n","model.add(Dense(100, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# 모델 컴파일\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# 모델 훈련\n","model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.1)\n","\n","# 모델 평가\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy*100:.2f}%\")\n","\n","\n","#=========================================================================\n","\n","\n","#자르는것\n","import librosa\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv1D, MaxPooling1D\n","from keras.utils import to_categorical\n","import glob\n","import os\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","# 음성 파일을 일정한 길이로 자르는 함수\n","def preprocess_audio(audio_files, target_length):\n","    processed_audios = []\n","    for audio_file in audio_files:\n","        try:\n","            y, sr = librosa.load(audio_file, sr=None)\n","\n","            if len(y) > target_length:\n","                # 음성 길이가 더 길 경우, 중간을 기준으로 양쪽을 잘라서 target_length로 만듭니다.\n","                start = (len(y) - target_length) // 2\n","                end = start + target_length\n","                y = y[start:end]\n","            elif len(y) < target_length:\n","                # 음성 길이가 더 짧을 경우, 앞뒤로 0으로 패딩하여 target_length로 만듭니다.\n","                padding = target_length - len(y)\n","                y = np.pad(y, (padding // 2, padding - padding // 2), mode='constant')\n","\n","            processed_audios.append(y)\n","        except Exception as e:\n","            print(f\"Error processing {audio_file}: {str(e)}\")\n","\n","    return processed_audios\n","\n","# 주파수 특성 추출 함수 (일정한 길이로 자른 음성 파일 사용)\n","max_length = 5  # 각 음성 파일의 목표 길이 (초)\n","\n","def extract_features(audio_files, max_length):\n","    features = []\n","    for audio_file in audio_files:\n","        try:\n","            audios = preprocess_audio([audio_file], int(max_length * sr))\n","\n","            for y in audios:\n","                mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n","                chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n","                spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n","\n","                window_features = np.concatenate((mfccs.flatten(), chroma.flatten(), spectral_contrast.flatten()))\n","\n","                features.append(window_features)\n","        except Exception as e:\n","            print(f\"Error processing {audio_file}: {str(e)}\")\n","\n","    # 특성을 배열로 변환\n","    features_array = np.array(features)\n","\n","    if len(features_array.shape) == 1:\n","        features_array = np.expand_dims(features_array, axis=0)\n","\n","    return features_array\n","\n","\n","\n","# 나머지 코드는 그대로 사용합니다.\n","\n","# 주어진 경로와 그 하위 경로에서 특정 형식의 파일들을 찾습니다.\n","def get_files_from_dir(path, file_format):\n","    return glob.glob(os.path.join(path, f'**/*.{file_format}'), recursive=True)\n","\n","ai_subfolders = ['sm', 'sy', 'mh']\n","ai_audio_files = []\n","for folder in ai_subfolders:\n","    ai_audio_files.extend(get_files_from_dir(f'/content/drive/MyDrive/PBL/ai_voice/ai/{folder}', 'mp3'))\n","\n","human_subfolders = ['cy', 'sy', 'hs']\n","human_audio_files=[]\n","for folder in human_subfolders:\n","  human_audio_files.extend(get_files_from_dir(f'/content/drive/MyDrive/PBL/ai_voice/human/{folder}', 'mp3'))\n","\n","# AI 음성 특징 추출 및 레이블 생성\n","ai_features_train_test_fullset = extract_features(ai_audio_files, max_length=max_length)\n","ai_labels_train_test_fullset = np.zeros(len(ai_features_train_test_fullset))\n","\n","# 사람 음성 특징 추출 및 레이블 생성\n","human_features_train_test_fullset = extract_features(human_audio_files, max_length=max_length)\n","# 두 배열 중 하나의 열 크기를 다른 배열과 일치시키기\n","if ai_features_train_test_fullset.shape[1] != human_features_train_test_fullset.shape[1]:\n","    target_shape = (human_features_train_test_fullset.shape[0], ai_features_train_test_fullset.shape[1])\n","    human_features_train_test_fullset = np.resize(human_features_train_test_fullset, target_shape)\n","\n","human_labels_train_test_fullset = np.ones(len(human_features_train_test_fullset))\n","\n","# 학습 데이터와 테스트 데이터 결합 및 레이블 결합\n","X_train_test = np.vstack((ai_features_train_test_fullset, human_features_train_test_fullset))\n","y_train_test = np.concatenate((ai_labels_train_test_fullset, human_labels_train_test_fullset))\n","\n","# 데이터를 학습 데이터와 검증 데이터로 분할\n","X_train, X_test, y_train, y_test = train_test_split(X_train_test, y_train_test, test_size=0.2)\n","\n","# 나머지 코드...\n","\n","# 데이터 형태 변경\n","X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n","y_train = to_categorical(y_train)\n","\n","X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n","y_test = to_categorical(y_test)\n","\n","# 모델 정의 및 컴파일\n","num_classes = len(set(np.concatenate((ai_labels_train_test_fullset, human_labels_train_test_fullset))))\n","\n","model = Sequential()\n","model.add(Conv1D(filters=32, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\", input_shape=(X_train.shape[1], 1)))\n","model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))\n","model.add(Flatten())\n","model.add(Dense(units=num_classes, activation=\"softmax\"))  # num_classes를 클래스 수로 설정\n","\n","# 모델 컴파일\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 모델 학습\n","history = model.fit(X_train, y_train, batch_size=16, epochs=30,\n","                    validation_data=(X_test,y_test))\n","\n","\n","#==============================================================================\n","\n","\n","# 모델 정의\n","model=Sequential()\n","model.add(Conv1D(filters=64,kernel_size=5,strides=1,padding=\"same\",activation=\"relu\",input_shape=(X_train.shape[1], 1)))\n","model.add(MaxPooling1D(pool_size=5,strides=2,padding='same'))\n","model.add(Conv1D(filters=32,kernel_size=5,strides=1,padding=\"same\",activation=\"relu\"))\n","model.add(MaxPooling1D(pool_size=5,strides=2,padding='same'))\n","model.add(Dropout(0.2))\n","model.add(Flatten())\n","model.add(Dense(units=len(set(np.concatenate((ai_labels, human_labels)))),activation=\"softmax\"))\n","\n","\n","# 모델 컴파일 및 학습\n","model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n","y_train = to_categorical(y_train)\n","\n","X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n","y_test = to_categorical(y_test)\n","\n","history=model.fit(X_train,y_train,batch_size=16,epochs=30,validation_data=(X_test,y_test))\n","\n","# 예측 및 결과 출력\n","test_audio_file = \"/content/drive/MyDrive/PBL/ai_voice/ai/ai_phone_call_noise_cancel.wav\"\n","test_features = extract_features(test_audio_file, max_length)\n","test_features = test_features.reshape(1, test_features.shape[0], 1)\n","prediction = model.predict(test_features)\n","predicted_class = np.argmax(prediction)  # 가장 높은 확률을 가진 클래스 선택\n","\n","confidence = prediction[0][predicted_class] * 100  # 선택된 클래스의 확률\n","\n","if predicted_class == 0:\n","    print(f\"테스트 음성은 AI 음성입니다. (확신도: {confidence:.2f}%)\")\n","else:\n","    print(f\"테스트 음성은 인간 음성입니다. (확신도: {confidence:.2f}%)\")\n","\n","\n","#=============================================================================\n","\n","\n","import matplotlib.pyplot as plt\n","\n","# Plot training & validation accuracy values\n","plt.figure(figsize=(12, 4))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","\n","# Plot training & validation loss values\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","\n","# Show the plot\n","plt.tight_layout()"]}]}
